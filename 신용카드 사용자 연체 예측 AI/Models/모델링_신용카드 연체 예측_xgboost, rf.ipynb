{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae3c1094",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "15edc40e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T07:27:26.267401Z",
     "start_time": "2021-12-02T07:27:26.253928Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import random\n",
    "import optuna\n",
    "import glob\n",
    "from optuna import Trial\n",
    "from optuna.samplers import TPESampler\n",
    "from sklearn.metrics import log_loss, classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold,train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c5ea4fd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T05:43:55.634053Z",
     "start_time": "2021-12-02T05:43:55.472927Z"
    }
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train_ver2.csv', index_col=0)\n",
    "test = pd.read_csv('../data/test_ver2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76627cd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T05:43:55.756116Z",
     "start_time": "2021-12-02T05:43:55.743136Z"
    }
   },
   "outputs": [],
   "source": [
    "train.reset_index(drop=True, inplace=True)\n",
    "test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0768d54",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T05:43:56.422982Z",
     "start_time": "2021-12-02T05:43:56.401797Z"
    }
   },
   "outputs": [],
   "source": [
    "X = train.drop([\"credit\"], axis=1)\n",
    "y = train[\"credit\"]\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe71449",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95939bfb",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 튜닝 - optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "00ca5019",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T06:12:56.472168Z",
     "start_time": "2021-12-02T06:12:56.463194Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective_xgb(trial: Trial) -> float:\n",
    "    params_xgb = {\n",
    "        \"random_state\": 42,\n",
    "        \"learning_rate\": trial.suggest_discrete_uniform('learning_rate', 0.01, 0.1, 0.01),\n",
    "        \"n_estimators\": trial.suggest_int('n_estimators', 0, 1000),\n",
    "        \"objective\": \"multiclass\",\n",
    "        \"metric\": \"multi_logloss\",\n",
    "        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-8, 3e-5),\n",
    "        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-8, 9e-2),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 20),\n",
    "        \"colsample_bytree\": trial.suggest_discrete_uniform('colsample_bytree', 0.5, 0.9, 0.1),\n",
    "        \"subsample\": trial.suggest_discrete_uniform('subsample', 0.5, 0.9, 0.1),\n",
    "    }\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    xgb = XGBClassifier(**params_xgb)\n",
    "    xgb.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "        early_stopping_rounds=30,\n",
    "        verbose=100,\n",
    "    )\n",
    "\n",
    "    xgb_pred = xgb.predict_proba(X_valid)\n",
    "    log_score = log_loss(y_valid, xgb_pred)\n",
    "    \n",
    "    return log_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e497a40f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T06:01:34.659080Z",
     "start_time": "2021-12-02T05:59:06.503103Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 14:59:06,504]\u001b[0m A new study created in memory with name: xgb_parameter_opt\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:59:06] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:59:06] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.08228\tvalidation_1-mlogloss:1.08250\n",
      "[100]\tvalidation_0-mlogloss:0.78763\tvalidation_1-mlogloss:0.80636\n",
      "[200]\tvalidation_0-mlogloss:0.75890\tvalidation_1-mlogloss:0.79082\n",
      "[300]\tvalidation_0-mlogloss:0.73542\tvalidation_1-mlogloss:0.77898\n",
      "[400]\tvalidation_0-mlogloss:0.71585\tvalidation_1-mlogloss:0.77049\n",
      "[500]\tvalidation_0-mlogloss:0.69869\tvalidation_1-mlogloss:0.76360\n",
      "[600]\tvalidation_0-mlogloss:0.68264\tvalidation_1-mlogloss:0.75831\n",
      "[700]\tvalidation_0-mlogloss:0.66816\tvalidation_1-mlogloss:0.75306\n",
      "[800]\tvalidation_0-mlogloss:0.65424\tvalidation_1-mlogloss:0.74806\n",
      "[900]\tvalidation_0-mlogloss:0.64112\tvalidation_1-mlogloss:0.74364\n",
      "[950]\tvalidation_0-mlogloss:0.63481\tvalidation_1-mlogloss:0.74138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 14:59:22,879]\u001b[0m Trial 0 finished with value: 0.7413744503749066 and parameters: {'learning_rate': 0.04, 'n_estimators': 951, 'reg_alpha': 2.196249831492404e-05, 'reg_lambda': 0.05387926759114846, 'max_depth': 4, 'colsample_bytree': 0.5, 'subsample': 0.5}. Best is trial 0 with value: 0.7413744503749066.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:59:22] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:59:23] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.03270\tvalidation_1-mlogloss:1.04975\n",
      "[61]\tvalidation_0-mlogloss:0.23176\tvalidation_1-mlogloss:0.74999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 14:59:32,129]\u001b[0m Trial 1 finished with value: 0.723628764196546 and parameters: {'learning_rate': 0.09, 'n_estimators': 601, 'reg_alpha': 2.1245096608103405e-05, 'reg_lambda': 0.0018526142807772773, 'max_depth': 20, 'colsample_bytree': 0.9, 'subsample': 0.6}. Best is trial 1 with value: 0.723628764196546.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:59:32] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:59:32] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.08851\tvalidation_1-mlogloss:1.08912\n",
      "[100]\tvalidation_0-mlogloss:0.73534\tvalidation_1-mlogloss:0.78494\n",
      "[182]\tvalidation_0-mlogloss:0.65867\tvalidation_1-mlogloss:0.74504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 14:59:40,203]\u001b[0m Trial 2 finished with value: 0.7450412114653712 and parameters: {'learning_rate': 0.02, 'n_estimators': 183, 'reg_alpha': 9.134224866356536e-06, 'reg_lambda': 0.04722808359933709, 'max_depth': 9, 'colsample_bytree': 0.6, 'subsample': 0.8}. Best is trial 1 with value: 0.723628764196546.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:59:40] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:59:40] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.08566\tvalidation_1-mlogloss:1.08875\n",
      "[100]\tvalidation_0-mlogloss:0.55817\tvalidation_1-mlogloss:0.73593\n",
      "[200]\tvalidation_0-mlogloss:0.39761\tvalidation_1-mlogloss:0.69419\n",
      "[276]\tvalidation_0-mlogloss:0.32845\tvalidation_1-mlogloss:0.69198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 15:00:04,184]\u001b[0m Trial 3 finished with value: 0.691593056951268 and parameters: {'learning_rate': 0.02, 'n_estimators': 292, 'reg_alpha': 1.0997191680377813e-05, 'reg_lambda': 0.04104630401883339, 'max_depth': 16, 'colsample_bytree': 0.5, 'subsample': 0.7}. Best is trial 3 with value: 0.691593056951268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:00:04] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:00:04] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.07240\tvalidation_1-mlogloss:1.07200\n",
      "[45]\tvalidation_0-mlogloss:0.81295\tvalidation_1-mlogloss:0.80881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 15:00:04,886]\u001b[0m Trial 4 finished with value: 0.8088142461067921 and parameters: {'learning_rate': 0.060000000000000005, 'n_estimators': 46, 'reg_alpha': 1.8230270108524137e-05, 'reg_lambda': 0.015347179426615001, 'max_depth': 2, 'colsample_bytree': 0.9, 'subsample': 0.9}. Best is trial 3 with value: 0.691593056951268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:00:04] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:00:04] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.05869\tvalidation_1-mlogloss:1.06183\n",
      "[100]\tvalidation_0-mlogloss:0.52738\tvalidation_1-mlogloss:0.72551\n",
      "[200]\tvalidation_0-mlogloss:0.40036\tvalidation_1-mlogloss:0.71751\n",
      "[201]\tvalidation_0-mlogloss:0.39955\tvalidation_1-mlogloss:0.71735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 15:00:13,207]\u001b[0m Trial 5 finished with value: 0.7150452131385406 and parameters: {'learning_rate': 0.09, 'n_estimators': 304, 'reg_alpha': 2.939186699051452e-06, 'reg_lambda': 0.061580975543763856, 'max_depth': 9, 'colsample_bytree': 0.5, 'subsample': 0.7}. Best is trial 3 with value: 0.691593056951268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:00:13] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:00:13] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.09394\tvalidation_1-mlogloss:1.09394\n",
      "[100]\tvalidation_0-mlogloss:0.85755\tvalidation_1-mlogloss:0.86090\n",
      "[200]\tvalidation_0-mlogloss:0.78387\tvalidation_1-mlogloss:0.79584\n",
      "[300]\tvalidation_0-mlogloss:0.74802\tvalidation_1-mlogloss:0.77175\n",
      "[400]\tvalidation_0-mlogloss:0.72323\tvalidation_1-mlogloss:0.75922\n",
      "[500]\tvalidation_0-mlogloss:0.70116\tvalidation_1-mlogloss:0.74985\n",
      "[600]\tvalidation_0-mlogloss:0.68130\tvalidation_1-mlogloss:0.74223\n",
      "[700]\tvalidation_0-mlogloss:0.66261\tvalidation_1-mlogloss:0.73530\n",
      "[800]\tvalidation_0-mlogloss:0.64565\tvalidation_1-mlogloss:0.72938\n",
      "[900]\tvalidation_0-mlogloss:0.63062\tvalidation_1-mlogloss:0.72499\n",
      "[909]\tvalidation_0-mlogloss:0.62922\tvalidation_1-mlogloss:0.72450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 15:00:47,471]\u001b[0m Trial 6 finished with value: 0.724500846904897 and parameters: {'learning_rate': 0.01, 'n_estimators': 910, 'reg_alpha': 7.770811648184508e-06, 'reg_lambda': 0.05962700896663554, 'max_depth': 7, 'colsample_bytree': 0.7, 'subsample': 0.7}. Best is trial 3 with value: 0.691593056951268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:00:47] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:00:47] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.08223\tvalidation_1-mlogloss:1.08786\n",
      "[100]\tvalidation_0-mlogloss:0.46008\tvalidation_1-mlogloss:0.74075\n",
      "[200]\tvalidation_0-mlogloss:0.30335\tvalidation_1-mlogloss:0.72409\n",
      "[203]\tvalidation_0-mlogloss:0.30024\tvalidation_1-mlogloss:0.72455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 15:01:14,731]\u001b[0m Trial 7 finished with value: 0.721294767444908 and parameters: {'learning_rate': 0.02, 'n_estimators': 970, 'reg_alpha': 2.3256233372599825e-05, 'reg_lambda': 0.0845549053457876, 'max_depth': 18, 'colsample_bytree': 0.7, 'subsample': 0.9}. Best is trial 3 with value: 0.691593056951268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:01:14] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:01:14] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.09363\tvalidation_1-mlogloss:1.09398\n",
      "[100]\tvalidation_0-mlogloss:0.84427\tvalidation_1-mlogloss:0.87170\n",
      "[195]\tvalidation_0-mlogloss:0.76274\tvalidation_1-mlogloss:0.81155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 15:01:22,621]\u001b[0m Trial 8 finished with value: 0.8115462687037613 and parameters: {'learning_rate': 0.01, 'n_estimators': 196, 'reg_alpha': 1.3663663944270366e-06, 'reg_lambda': 0.029279736515390484, 'max_depth': 8, 'colsample_bytree': 0.6, 'subsample': 0.9}. Best is trial 3 with value: 0.691593056951268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15:01:22] WARNING: ..\\src\\learner.cc:541: \n",
      "Parameters: { metric } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:01:22] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.07038\tvalidation_1-mlogloss:1.07846\n",
      "[100]\tvalidation_0-mlogloss:0.34923\tvalidation_1-mlogloss:0.72486\n",
      "[122]\tvalidation_0-mlogloss:0.30652\tvalidation_1-mlogloss:0.72991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 15:01:34,652]\u001b[0m Trial 9 finished with value: 0.7244031605557343 and parameters: {'learning_rate': 0.04, 'n_estimators': 281, 'reg_alpha': 1.6285455533915874e-05, 'reg_lambda': 0.01268318883848639, 'max_depth': 17, 'colsample_bytree': 0.5, 'subsample': 0.9}. Best is trial 3 with value: 0.691593056951268.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.691593056951268\n",
      "Best trial {'learning_rate': 0.02, 'n_estimators': 292, 'reg_alpha': 1.0997191680377813e-05, 'reg_lambda': 0.04104630401883339, 'max_depth': 16, 'colsample_bytree': 0.5, 'subsample': 0.7}\n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(\n",
    "    study_name = 'xgb_parameter_opt',\n",
    "    direction = 'minimize',\n",
    "    sampler = sampler,\n",
    ")\n",
    "study.optimize(objective_xgb, n_trials=10)\n",
    "print(\"Best Score:\",study.best_value)\n",
    "print(\"Best trial\",study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d445e884",
   "metadata": {},
   "source": [
    "### 10-Fold + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8801217f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T06:27:24.300606Z",
     "start_time": "2021-12-02T06:27:24.281670Z"
    }
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "folds = []\n",
    "for train_idx, valid_idx in skf.split(train, train['credit']):\n",
    "    folds.append((train_idx,valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d65cd6f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T06:32:20.258160Z",
     "start_time": "2021-12-02T06:27:53.388549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================1============================================\n",
      "[15:27:53] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.08683\tvalidation_1-mlogloss:1.08936\n",
      "[100]\tvalidation_0-mlogloss:0.55497\tvalidation_1-mlogloss:0.73509\n",
      "[200]\tvalidation_0-mlogloss:0.40527\tvalidation_1-mlogloss:0.69271\n",
      "[264]\tvalidation_0-mlogloss:0.34324\tvalidation_1-mlogloss:0.69070\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================2============================================\n",
      "[15:28:17] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.08577\tvalidation_1-mlogloss:1.08914\n",
      "[100]\tvalidation_0-mlogloss:0.55527\tvalidation_1-mlogloss:0.74666\n",
      "[200]\tvalidation_0-mlogloss:0.40408\tvalidation_1-mlogloss:0.71152\n",
      "[271]\tvalidation_0-mlogloss:0.33919\tvalidation_1-mlogloss:0.71126\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================3============================================\n",
      "[15:28:43] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.08589\tvalidation_1-mlogloss:1.08868\n",
      "[100]\tvalidation_0-mlogloss:0.56316\tvalidation_1-mlogloss:0.73889\n",
      "[200]\tvalidation_0-mlogloss:0.40653\tvalidation_1-mlogloss:0.69403\n",
      "[291]\tvalidation_0-mlogloss:0.32728\tvalidation_1-mlogloss:0.68964\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================4============================================\n",
      "[15:29:12] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.08554\tvalidation_1-mlogloss:1.08865\n",
      "[100]\tvalidation_0-mlogloss:0.56039\tvalidation_1-mlogloss:0.74480\n",
      "[200]\tvalidation_0-mlogloss:0.40610\tvalidation_1-mlogloss:0.70883\n",
      "[256]\tvalidation_0-mlogloss:0.35461\tvalidation_1-mlogloss:0.70868\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================5============================================\n",
      "[15:29:38] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.08612\tvalidation_1-mlogloss:1.08895\n",
      "[100]\tvalidation_0-mlogloss:0.55901\tvalidation_1-mlogloss:0.73994\n",
      "[200]\tvalidation_0-mlogloss:0.40868\tvalidation_1-mlogloss:0.69859\n",
      "[283]\tvalidation_0-mlogloss:0.33700\tvalidation_1-mlogloss:0.69446\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================6============================================\n",
      "[15:30:05] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.08596\tvalidation_1-mlogloss:1.08890\n",
      "[100]\tvalidation_0-mlogloss:0.56027\tvalidation_1-mlogloss:0.74897\n",
      "[200]\tvalidation_0-mlogloss:0.40773\tvalidation_1-mlogloss:0.71672\n",
      "[248]\tvalidation_0-mlogloss:0.36035\tvalidation_1-mlogloss:0.71669\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================7============================================\n",
      "[15:30:29] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.08606\tvalidation_1-mlogloss:1.08877\n",
      "[100]\tvalidation_0-mlogloss:0.55916\tvalidation_1-mlogloss:0.74004\n",
      "[200]\tvalidation_0-mlogloss:0.40370\tvalidation_1-mlogloss:0.69949\n",
      "[286]\tvalidation_0-mlogloss:0.32899\tvalidation_1-mlogloss:0.69482\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================8============================================\n",
      "[15:30:59] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.08651\tvalidation_1-mlogloss:1.08893\n",
      "[100]\tvalidation_0-mlogloss:0.56095\tvalidation_1-mlogloss:0.74559\n",
      "[200]\tvalidation_0-mlogloss:0.40514\tvalidation_1-mlogloss:0.70645\n",
      "[279]\tvalidation_0-mlogloss:0.33573\tvalidation_1-mlogloss:0.70616\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================9============================================\n",
      "[15:31:26] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.08530\tvalidation_1-mlogloss:1.08854\n",
      "[100]\tvalidation_0-mlogloss:0.55453\tvalidation_1-mlogloss:0.74135\n",
      "[200]\tvalidation_0-mlogloss:0.40309\tvalidation_1-mlogloss:0.70899\n",
      "[244]\tvalidation_0-mlogloss:0.36215\tvalidation_1-mlogloss:0.70819\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================10============================================\n",
      "[15:31:51] WARNING: ..\\src\\learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0]\tvalidation_0-mlogloss:1.08618\tvalidation_1-mlogloss:1.08877\n",
      "[100]\tvalidation_0-mlogloss:0.56338\tvalidation_1-mlogloss:0.74516\n",
      "[200]\tvalidation_0-mlogloss:0.40991\tvalidation_1-mlogloss:0.70701\n",
      "[271]\tvalidation_0-mlogloss:0.34458\tvalidation_1-mlogloss:0.70666\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "xgb_models={}\n",
    "\n",
    "for fold in range(10):\n",
    "    print(f'===================================={fold+1}============================================')\n",
    "    train_idx, valid_idx = folds[fold]\n",
    "    X_train = train.drop(['credit'],axis=1).iloc[train_idx].values \n",
    "    X_valid = train.drop(['credit'],axis=1).iloc[valid_idx].values\n",
    "    y_train = train['credit'][train_idx].values\n",
    "    y_valid = train['credit'][valid_idx].values\n",
    "\n",
    "    xgb = XGBClassifier(**study.best_params)\n",
    "    xgb.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "        early_stopping_rounds=30,\n",
    "        verbose=100,\n",
    "    )\n",
    "    xgb_models[fold] = xgb\n",
    "    print(f'================================================================================\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1779734",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T07:08:20.458683Z",
     "start_time": "2021-12-02T07:08:20.447243Z"
    }
   },
   "outputs": [],
   "source": [
    "submit = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4eeab751",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T07:08:23.437988Z",
     "start_time": "2021-12-02T07:08:22.151294Z"
    }
   },
   "outputs": [],
   "source": [
    "submit.iloc[:,1:]=0\n",
    "for fold in range(10):\n",
    "    submit.iloc[:,1:] += xgb_models[fold].predict_proba(test)/10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f6bf441",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T07:08:31.320294Z",
     "start_time": "2021-12-02T07:08:31.287354Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9998.00000</td>\n",
       "      <td>9998.000000</td>\n",
       "      <td>9998.000000</td>\n",
       "      <td>9998.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>31455.50000</td>\n",
       "      <td>0.113125</td>\n",
       "      <td>0.207456</td>\n",
       "      <td>0.679419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2886.31833</td>\n",
       "      <td>0.068474</td>\n",
       "      <td>0.160625</td>\n",
       "      <td>0.183808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>26457.00000</td>\n",
       "      <td>0.019798</td>\n",
       "      <td>0.023012</td>\n",
       "      <td>0.024428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>28956.25000</td>\n",
       "      <td>0.072517</td>\n",
       "      <td>0.119283</td>\n",
       "      <td>0.636246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>31455.50000</td>\n",
       "      <td>0.094281</td>\n",
       "      <td>0.158578</td>\n",
       "      <td>0.729430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>33954.75000</td>\n",
       "      <td>0.129920</td>\n",
       "      <td>0.222848</td>\n",
       "      <td>0.790420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>36454.00000</td>\n",
       "      <td>0.733110</td>\n",
       "      <td>0.940980</td>\n",
       "      <td>0.957190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             index            0            1            2\n",
       "count   9998.00000  9998.000000  9998.000000  9998.000000\n",
       "mean   31455.50000     0.113125     0.207456     0.679419\n",
       "std     2886.31833     0.068474     0.160625     0.183808\n",
       "min    26457.00000     0.019798     0.023012     0.024428\n",
       "25%    28956.25000     0.072517     0.119283     0.636246\n",
       "50%    31455.50000     0.094281     0.158578     0.729430\n",
       "75%    33954.75000     0.129920     0.222848     0.790420\n",
       "max    36454.00000     0.733110     0.940980     0.957190"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "119e491c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T07:08:38.453657Z",
     "start_time": "2021-12-02T07:08:38.392235Z"
    }
   },
   "outputs": [],
   "source": [
    "submit.to_csv('../data/submit_xgb.csv', index=False) # test 데이터 전처리 과정에서 row 2개가 빠져 평균값으로 채워주었다 (대회 score 제출 규정)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce312db0",
   "metadata": {},
   "source": [
    "## RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536e02bb",
   "metadata": {},
   "source": [
    "### 하이퍼파라미터 튜닝 - optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "df700534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T07:37:42.118152Z",
     "start_time": "2021-12-02T07:37:42.110608Z"
    }
   },
   "outputs": [],
   "source": [
    "def objective_rf(trial: Trial) -> float:\n",
    "    params_rf = {\n",
    "        \"random_state\": 42,\n",
    "        \"n_estimators\": trial.suggest_int('n_estimators', 50, 1000),\n",
    "        \"max_depth\": trial.suggest_int('max_depth', 4, 50),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 1, 150),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 60),\n",
    "    }\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "    rf = RandomForestClassifier(**params_rf)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    rf_pred = rf.predict_proba(X_valid)\n",
    "    log_score = log_loss(y_valid, rf_pred)\n",
    "    \n",
    "    return log_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cbbfd1cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T07:39:13.394375Z",
     "start_time": "2021-12-02T07:37:46.527117Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-12-02 16:37:46,529]\u001b[0m A new study created in memory with name: rf_parameter_opt\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 16:37:55,193]\u001b[0m Trial 0 finished with value: 0.7908607815252745 and parameters: {'n_estimators': 406, 'max_depth': 48, 'min_samples_split': 110, 'min_samples_leaf': 36}. Best is trial 0 with value: 0.7908607815252745.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 16:37:58,616]\u001b[0m Trial 1 finished with value: 0.7853724802341039 and parameters: {'n_estimators': 198, 'max_depth': 11, 'min_samples_split': 9, 'min_samples_leaf': 52}. Best is trial 1 with value: 0.7853724802341039.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 16:38:10,676]\u001b[0m Trial 2 finished with value: 0.7989344695596716 and parameters: {'n_estimators': 621, 'max_depth': 37, 'min_samples_split': 4, 'min_samples_leaf': 59}. Best is trial 1 with value: 0.7853724802341039.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 16:38:28,373]\u001b[0m Trial 3 finished with value: 0.7874047977497097 and parameters: {'n_estimators': 841, 'max_depth': 13, 'min_samples_split': 28, 'min_samples_leaf': 12}. Best is trial 1 with value: 0.7853724802341039.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 16:38:36,294]\u001b[0m Trial 4 finished with value: 0.7791401764941243 and parameters: {'n_estimators': 339, 'max_depth': 28, 'min_samples_split': 65, 'min_samples_leaf': 18}. Best is trial 4 with value: 0.7791401764941243.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 16:38:47,674]\u001b[0m Trial 5 finished with value: 0.7911001057203566 and parameters: {'n_estimators': 631, 'max_depth': 10, 'min_samples_split': 44, 'min_samples_leaf': 22}. Best is trial 4 with value: 0.7791401764941243.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 16:38:58,593]\u001b[0m Trial 6 finished with value: 0.7784112114929467 and parameters: {'n_estimators': 483, 'max_depth': 40, 'min_samples_split': 30, 'min_samples_leaf': 31}. Best is trial 6 with value: 0.7784112114929467.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 16:39:06,051]\u001b[0m Trial 7 finished with value: 0.8172340766116727 and parameters: {'n_estimators': 613, 'max_depth': 6, 'min_samples_split': 92, 'min_samples_leaf': 11}. Best is trial 6 with value: 0.7784112114929467.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 16:39:08,257]\u001b[0m Trial 8 finished with value: 0.7952624465400634 and parameters: {'n_estimators': 111, 'max_depth': 48, 'min_samples_split': 145, 'min_samples_leaf': 49}. Best is trial 6 with value: 0.7784112114929467.\u001b[0m\n",
      "\u001b[32m[I 2021-12-02 16:39:13,376]\u001b[0m Trial 9 finished with value: 0.821530837014175 and parameters: {'n_estimators': 339, 'max_depth': 8, 'min_samples_split': 103, 'min_samples_leaf': 27}. Best is trial 6 with value: 0.7784112114929467.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7784112114929467\n",
      "Best trial {'n_estimators': 483, 'max_depth': 40, 'min_samples_split': 30, 'min_samples_leaf': 31}\n"
     ]
    }
   ],
   "source": [
    "sampler = TPESampler(seed=42)\n",
    "study = optuna.create_study(\n",
    "    study_name = 'rf_parameter_opt',\n",
    "    direction = 'minimize',\n",
    "    sampler = sampler,\n",
    ")\n",
    "study.optimize(objective_rf, n_trials=10)\n",
    "print(\"Best Score:\",study.best_value)\n",
    "print(\"Best trial\",study.best_trial.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1d4328",
   "metadata": {},
   "source": [
    "### 10-Fold + XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e474d0b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T07:54:04.551465Z",
     "start_time": "2021-12-02T07:54:04.537474Z"
    }
   },
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "folds = []\n",
    "for train_idx, valid_idx in skf.split(train, train['credit']):\n",
    "    folds.append((train_idx,valid_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8cb2be87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-02T07:56:58.373902Z",
     "start_time": "2021-12-02T07:54:48.165364Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================1============================================\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================2============================================\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================3============================================\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================4============================================\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================5============================================\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================6============================================\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================7============================================\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================8============================================\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================9============================================\n",
      "================================================================================\n",
      "\n",
      "\n",
      "====================================10============================================\n",
      "================================================================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "rf_models={}\n",
    "\n",
    "for fold in range(10):\n",
    "    print(f'===================================={fold+1}============================================')\n",
    "    train_idx, valid_idx = folds[fold]\n",
    "    X_train = train.drop(['credit'],axis=1).iloc[train_idx].values \n",
    "    X_valid = train.drop(['credit'],axis=1).iloc[valid_idx].values\n",
    "    y_train = train['credit'][train_idx].values\n",
    "    y_valid = train['credit'][valid_idx].values\n",
    "\n",
    "    rf = RandomForestClassifier(**study.best_params)\n",
    "    rf.fit(X_train, y_train)\n",
    "    rf_models[fold] = rf\n",
    "    print(f'================================================================================\\n\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
